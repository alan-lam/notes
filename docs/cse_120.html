<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
    -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <title>Operating Systems</title>
    <link href="cse_120.css" rel="stylesheet">
  </head>
  <body>
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top">
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="index.html" target="_blank">Home</a>
          </li>
          <!-- <li class="nav-item">
            <a class="nav-link" href="#" target="_blank">Cheat Sheets</a>
          </li> -->
        </ul>
      </div>
    </nav>
    <div class="toc">
      <ul>
        <a class="toc_link" href="#processes"><li>Processes</li></a>
        <a class="toc_link" href="#timesharing"><li>Timesharing</li></a>
        <a class="toc_link" href="#scheduling"><li>Scheduling</li></a>
        <a class="toc_link" href="#synchronization"><li>Synchronization</li></a>
        <a class="toc_link" href="#ipc"><li>InterProcess Communication</li></a>
        <a class="toc_link" href="#deadlock"><li>Deadlock</li></a>
        <a class="toc_link" href="#memory_management"><li>Memory Management</li></a>
        <a class="toc_link" href="#logical_memory"><li>Logical Memory</li></a>
        <a class="toc_link" href="#virtual_memory"><li>Virtual Memory</li></a>
        <a class="toc_link" href="#file_system"><li>File System</li></a>
        <a class="toc_link" href="#io"><li>Input/Output System</li></a>
        <a class="toc_link" href="#protection"><li>Protection</li></a>
        <a class="toc_link" href="#security"><li>Security</li></a>
        <a class="toc_link" href="#networks"><li>Networks</li></a>
        <a class="toc_link" href="#distributed_systems"><li>Distributed Systems</li></a>
      </ul>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-md-9">
          <h1>Operating Systems</h1>
          <h6 class="tab">Professor Joseph Pasquale, CSE 120, Winter 2018</h2>
          <div class="content" id="processes">
            <h3>Processes</h3>
            <ul>
              <li><p>Most basic function of kernel is to run programs</p></li>
              <li><p>How do we achieve that with just 1 (or small number) cpu and memory?</p></li>
              <li><p>Want to create illusion that there are many cpus; 1 for every program</p></li>
              <li><p><b>A process is a program in execution</b></p></li>
              <li style="list-style-type: none";><p>- A program is static; a process is the activity of executing that program</p></li>
              <li><p>Processes have states that change over time</p></li>
              <li><p>Processes need resources (CPU, memory, I/O, ...) to actually execute</p></li>
              <li><p>Processes have contexts - all of the machine and kernel-related states</p></li>
              <li><p>A context is comprised of CPU context (values of registers, stack pointer, program counter, ...) and memory context (code, variables, things in memory, ...)</p></li>
              <li><p><b>A process has 3 memory areas: stack, text, data</b></p></li>
              <li><p><b>Text contains all of the code</b></p></li>
              <li><p><b>Data contains global variables, heap</b></p></li>
              <li><p><b>Stack contains activation records</b></p></li>
              <img class="img-fluid" src="./pictures/text-data-stack (processes).jpg">
              <li><p>An activation record is information pertaining to a procedure call</p></li>
              <li style="list-style-type: none";><p>- If a procedure calls itself 3 times, there will be 3 activation records</p></li>
              <li><p>Activation records store:</p></li>
              <li style="list-style-type: none";><p>- pointer where to return to</p></li>
              <li style="list-style-type: none";><p>- link to previous record (so we know where the new top of the stack is after this is popped off the stack)</p></li>
              <li style="list-style-type: none";><p>- local variables</p></li>
              <li><p>Stack pointer register contains the location of the top of the stack</p></li>
              <li><p>Return address keeps track of where to return to after procedure</p></li>
              <li><p>All this is for 1 process; but we want to run many processes</p></li>
              <li><p>In reality, if only 1 cpu, only 1 process could be running</p></li>
              <li><p><b>Multiprogramming: voluntarily giving up CPU to another process</b></p></li>
              <li><p>A process running may need a resource (keystroke, anything needed to get work done). If it doesn't get that resource, the process is just sitting there doing nothing. <b>It can't make any progress, so let's give CPU to another process that can</b></p></li>
              <li><p>yield(p) - give CPU to process p</p></li>
              <li><p><b>Context switching: allocating of CPU from one process to another</b></p></li>
              <li style="list-style-type: none";><p>- Save the context of one process, restore context of process we want to run</p></li>
              <li><p><b>Switch text, data, and stack because each process has its own text, data, and stack</b></p></li>
              <li><p>Yield is so important, we don't want it written by programmers. We want it written by OS implementer and put in the kernel</p></li>
              <li><p>Kernel is code that executes. Is it a process then? No it is not!</p></li>
              <li style="list-style-type: none";><p>- Kernel supports processes; can be thought of as an extension of all processes. Processes run inside the kernel.</p></li>
              <li><p><b>Kernel has its own text and data. It has 1 stack per process</b></p></li>
              <li><p><b>Yield is in the kernel, so calling yield causes a jump into the kernel</b></p></li>
              <li><p>Yield is a <b>system call - a function in the kernel that is callable by a process</b></p></li>
            </ul>
          </div>
          <div class="content" id="timesharing">
            <h3>Timesharing</h3>
            <ul>
              <li><p>If a process doesn't give up CPU, how does the system keep working?</p></li>
              <li><p>Quantum: amount of time a process runs</p></li>
              <li><p><b>Each process is getting a quantum of CPU time. This creates the illusion of parallel progress by rapidly switching CPU</b></p></li>
            </ul>
            <h5>How Timesharing Is Implemented</h5>
            <ul>
              <li><p>Kernel keeps track of progress of each process and characterizes their states</p></li>
              <li style="list-style-type: none";><p>- <b>Running: actually using CPU</b></p></li>
              <li style="list-style-type: none";><p>- <b>Ready: able to use CPU, but not using</b></p></li>
              <li style="list-style-type: none";><p>- <b>Blocked: does not have CPU, not able to run</b></p></li>
              <li><p>Running -> Blocked (sleep): when process tries to access a resource, but the resource is being used by another process</p></li>
              <li><p>Blocked -> Ready (wake up): when resource it was waiting for becomes available</p></li>
              <li><p>Ready -> Running (dispatch): when CPU is given to process by kernel</p></li>
              <li><p>Running -> Ready (preempt): if process never asks for a resource</p></li>
            </ul>
            <h5>Process vs. Kernel</h5>
            <ul>
              <li><p>Kernel includes functionality for system calls</p></li>
              <li><p><b>Kernel runs when process makes a system call or a hardware interrupt occurs</b></p></li>
              <li><p><b>When process is running its own code, it runs in user space. When it makes a system call, it jumps into kernel space</b></p></li>
              <li><p>While running in kernel space, variables are accessible via the kernel's data area and stack</p></li>
              <li><p>Within the kernel, stuff in kernel space and user space is accessible. In user space, only the user space stuff is accessible</p></li>
            </ul>
            <h5>Kernel Maintains List of Processes</h5>
            <ul>
              <li><p>Kernel has a process table of unique process ID's and their states</p></li>
              <li>The table also has contents of all CPU contexts, areas of memory being used, reasons for being blocked</li>
            </ul>
            <h5>How Kernel Gets Control</h5>
            <ul>
              <li><p><b>Kernel can get control when process voluntarily gives up control by making a system call that blocks</b></p></li>
              <li><p><b>Kernel can take away CPU from currently running process - preemption</b></p></li>
              <li><p><b>There is a hardware clock that is programmed to go off at a certain time. When it goes off - that's a hardware interrupt - and the kernel gets control</b></p></li>
              <li style="list-style-type: none";><p>- When kernel gives control to a new process, it resets timer</p></li>
            </ul>
            <h5>How a Context Switch Occurs</h5>
            <ul>
              <li><p>Process makes system call or hardware interrupt occurs to get into kernel</p></li>
              <li><p>Kernel expects hardware to do some things on its behalf:</p></li>
              <li style="list-style-type: none";><p>- hardware switches from user mode to kernel mode (characteristics of hardware)</p></li>
              <li style="list-style-type: none";><p>- in user mode, process has less power because it can't execute all instructions</p></li>
              <li style="list-style-type: none";><p>- by going into kernel mode, its power is amplified</p></li>
              <li style="list-style-type: none";><p>- hardware arranges for control to go to a fixed location, which is the handler: the code that runs to handle the trap (system call) or interrupt</p></li>
              <li><p>Kernel now saves context of currently running process, selects a ready process, restores its context, and returns from interrupt/trap</p></li>
              <li><p>rti instruction: return from interrupt; need a special return because going from kernel space to user space; rti turns kernel mode off</p></li>
              <li><p>Kernel has access to all processes, but only 1 process is running at a time</p></li>
            </ul>
            <h5>How To Get Parallelism Within a Process</h5>
            <ul>
              <li><p>What if we want multiple things going on within the same process?</p></li>
              <li><p><b>Want a single text and data, but multiple stacks for every path of execution</b></p></li>
              <li><p><b>Thread: single sequential path of execution</b></p></li>
              <li style="list-style-type: none";><p>- <b>like a process, but no memory; lives in a process</b></p></li>
              <li><p>Can have multiple threads in a process</p></li>
              <li><p>To the user: a thread is a unit of parallelism; more threads -> more parallelism</p></li>
              <li><p>To the kernel: a thread is a unit of schedulability; it is an object that the kernel assigns to a CPU</p></li>
              <li><p><b>User level thread: support for threads is part of the program; implemented by programmer or by including a thread library; kernel thinks it is a single process</b></p></li>
              <li><p><b>Kernel level thread: support for threads is provided by kernel; kernel can put threads on separate CPU's</b></p></li>
              <li><p>User level picture: each process has 1 stack for each thread; each <em>process</em> has code for context switching, scheduling; kernel has 1 stack for each <em>process</em></p></li>
              <li><p>Kernel level picture: each process has 1 stack for each thread; <em>kernel</em> has code for context switching, scheduling; kernel has 1 stack for each <em>thread</em></p></li>
            </ul>
            <h5>Pros And Cons</h5>
            <ul>
              <li><p>Pros of user level threads:</p></li>
              <li style="list-style-type: none";><p>- Programmers should write programs independent of OS; portability - works on any kernel</p></li>
              <li style="list-style-type: none";><p>- Context switching in user space is efficient; going into kernel takes time</p></li>
              <li style="list-style-type: none";><p>- Programmer can control scheduling policy</p></li>
              <li><p>Cons of user level threads:</p></li>
              <li style="list-style-type: none";><p>- No true parallelism</p></li>
              <li><p>Pros of kernel level threads:</p></li>
              <li style="list-style-type: none";><p>- true parallelism - different thread on each CPU</p></li>
              <li><p>Cons of kernel level threads:</p></li>
              <li style="list-style-type: none";><p>- overhead - jumping into kernel is expensive</p></li>
            </ul>
          </div>
          <div class="content" id="scheduling">
            <h3>Scheduling</h3>
            <ul>
              <li><p>CPU scheduling problem: if there are multiple processes and only 1 CPU, how much time should each process get?</p></li>
              <li><p>No single best policy; depends on goals of the system</p></li>
              <li style="list-style-type: none";><p>- personal computer: active window gets most CPU</p></li>
              <li style="list-style-type: none";><p>- large time-shared computer: everyone gets equal amount</p></li>
              <li><p>Arrival time: when process is created</p></li>
              <li><p>Service time: amount of CPU time for a process</p></li>
              <li><p>Turnaround time: time between arriving and departing</p></li>
              <li style="list-style-type: none";><p>- want to be as small as possible</p></li>
            </ul>
            <h5>Processes Arrive at the Same Time</h5>
            <ul>
              <li><p>Longest First: order processes by amount of CPU time needed from greatest to least</p></li>
              <li><p>Shortest First: order from least time needed to greatest time needed</p></li>
              <li style="list-style-type: none";><p>- provably optimal</p></li>
              <li style="list-style-type: none";><p>Longest First</p></li>
              <table>
                <tr id="axis">
                  <td></td>
                  <td>1</td>
                  <td>2</td>
                  <td>3</td>
                  <td>4</td>
                  <td>5</td>
                  <td>6</td>
                  <td>7</td>
                  <td>8</td>
                  <td>9</td>
                  <td>T T</td>
                </tr>
                <tr>
                  <td>P<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td>5</td>
                </tr>
                <tr>
                  <td>P<sub>2</sub></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="active">S<sub>2</sub></td>
                  <td id="active">S<sub>2</sub></td>
                  <td id="active">S<sub>2</sub></td>
                  <td></td>
                  <td>8</td>
                </tr>
                <tr>
                  <td>P<sub>3</sub></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="active">S<sub>3</sub></td>
                  <td>9</td>
                </tr>
              </table>
              <p>T T: turnaround time</p>
              <p>Average Turnaround Time: 22/3</p>
              <p>Process 1, Process 2, Process 3 all arrive at the same time. P<sub>1</sub> is scheduled first and it has service time S<sub>1</sub>. P<sub>2</sub> and P<sub>3</sub> are waiting for P<sub>1</sub> to finish. Once P<sub>1</sub> is done, P<sub>2</sub>, which is the next longest process, is scheduled. Once P<sub>2</sub> is done, P<sub>3</sub> runs.</p>
              <p>P<sub>2</sub>, P<sub>3</sub>, ..., P<sub>n</sub> all have to wait S<sub>1</sub> time. So it is best to make it as small as possible. Hence, shortest first.</p>
              <li style="list-style-type: none";><p>Shortest First</p></li>
              <table>
                <tr id="axis">
                  <td></td>
                  <td>1</td>
                  <td>2</td>
                  <td>3</td>
                  <td>4</td>
                  <td>5</td>
                  <td>6</td>
                  <td>7</td>
                  <td>8</td>
                  <td>9</td>
                  <td>T T</td>
                </tr>
                <tr>
                  <td>P<sub>1</sub></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="waiting"></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td id="active">S<sub>1</sub></td>
                  <td>9</td>
                </tr>
                <tr>
                  <td>P<sub>2</sub></td>
                  <td id="waiting"></td>
                  <td id="active">S<sub>2</sub></td>
                  <td id="active">S<sub>2</sub></td>
                  <td id="active">S<sub>2</sub></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td>4</td>
                </tr>
                <tr>
                  <td>P<sub>3</sub></td>
                  <td id="active">S<sub>3</sub></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td></td>
                  <td>1</td>
                </tr>
              </table>
              <p>T T: turnaround time</p>
              <p>Average Turnaround Time: 14/3</p>
              <li><p>Math proof of shortest first being optimal:</p></li>
              <li style="list-style-type: none";><p>Average T T = [S<sub>1</sub> + (S<sub>1</sub> + S<sub>2</sub>) + ... + (S<sub>1</sub> + ... + S<sub>n</sub>)] / n</p></li>
              <li style="list-style-type: none";><p>= [nS<sub>1</sub> + (n-1)S<sub>2</sub> + ... + S<sub>n</sub>] / n</p></li>
              <li style="list-style-type: none";><p>S<sub>1</sub> has the most weight (n) so it should be as small as possible to minimize average turnaround time.</p></li>
            </ul>
            <h5>Processes Arrive at Different Times</h5>
            <ul>
              <li><p>Non-preemptive: CPU is not taken away from a process once its given</p></li>
              <li><p>Starvation: when a process wants a resource but can't get it</p></li>
              <li><p>First Come First Served: first process created gets all the CPU, then the next process created gets the CPU when the previous process is done</p></li>
              <li style="list-style-type: none";><p>- non-preemptive</p></li>
              <li style="list-style-type: none";><p>- no starvation (all processes eventually get CPU time)</p></li>
              <li style="list-style-type: none";><p>- bad for short processes (have to wait for long processes to finish if they (long processes) are created first)</p></li>
              <li><p>Round Robin: every process gets a time-slice (quantum) of CPU</p></li>
              <li style="list-style-type: none";><p>- generally does better than FCFS; a process waits at most (n-1)*quantum units of time</p></li>
              <li style="list-style-type: none";><p>- preemptive</p></li>
              <li style="list-style-type: none";><p>- no starvation</p></li>
              <li><p>Shortest Process Next: select the shortest process to run next</p></li>
              <li style="list-style-type: none";><p>- theoretical; have to know how long processes will run ahead of time</p></li>
              <li style="list-style-type: none";><p>- most optimal non-preemptive policy</p></li>
              <li style="list-style-type: none";><p>- allows for starvation; long process is waiting, but shorter processes keep coming in</p></li>
              <li><p>Shortest Remaining Time: select process with shortest remaining time to run next</p></li>
              <li style="list-style-type: none";><p>- theoretical</p></li>
              <li style="list-style-type: none";><p>- most optimal preemptive policy</p></li>
              <li style="list-style-type: none";><p>- allows for starvation</p></li>
              <li><p>Multi-Level Feedback Queues</p></li>
              <li style="list-style-type: none";><p>- levels represent priority; processes in queue 0 have the highest priority and processes in queue n have the lowest priority</p></li>
              <li style="list-style-type: none";><p>- new processes enter queue 0</p></li>
              <li style="list-style-type: none";><p>- select process with highest priority</p></li>
              <li style="list-style-type: none";><p>- process runs for 2<sup>k</sup> quantums, where k is the level of the queue</p></li>
              <li style="list-style-type: none";><p>- higher priority processes get the CPU more often, but have shorter running times; lower priority processes don't get the CPU as often, but have longer running times</p></li>
              <li style="list-style-type: none";><p>- if a process ran for less than 2<sup>k</sup> quantums (CPU gets taken away before it could use all the time it was given), it gets put back in queue k</p></li>
              <li style="list-style-type: none";><p>- if a process ran for 2<sup>k</sup> quantums (able to use all the time it was given), it gets put in queue k+1</p></li>
              <li style="list-style-type: none";><p>- potential starvation; fix by periodically raising everyone's priority</p></li>
              <li style="list-style-type: none";><p>- favors short processes, so it does well</p></li>
              <li style="list-style-type: none";><p>- learns over time; sorts processes</p></li>
              <li><p>Priority Scheduling: label each process with a priority and select the highest one</p></li>
              <li style="list-style-type: none";><p>- tie priorities to something external (1/CPU time used)</p></li>
              <li><p>Fair Share (Proportional Share): every process gets a proportion of what they ask for</p></li>
              <li style="list-style-type: none";><p>- select process with minimum actual/request ratio</p></li>
              <li style="list-style-type: none";><p>- inefficient because have to compute fractions and minimum at every quantum</p></li>
            </ul>
            <h5>Real Time Scheduling</h5>
            <ul>
              <li><p>Real time system is correct if computations are correct and done within a certain amount of time</p></li>
              <li><p>Non-real time system only needs to make correct computations</p></li>
              <li><p>Hard real time system must meet all deadlines (e.g. nuclear reactor)</p></li>
              <li><p>Soft real time system - ok to miss a few deadlines</p></li>
              <li><p>Why not use hard real time for everything?</p></li>
              <li style="list-style-type: none";><p>- requires a lot of resources; might not be able to run anything else</p></li>
            </ul>
            <h5>Periodic Tasks</h5>
            <ul>
              <li><p>Periodic: schedule repeats every period</p></li>
              <li><p>Can processes be ordered to meet all deadlines?</p></li>
              <li><p>Earliest Deadline First: schedule process with earliest deadline first</p></li>
              <li style="list-style-type: none";><p>- works for periodic and aperiodic processes</p></li>
              <li style="list-style-type: none";><p>- expensive - sort deadlines</p></li>
              <li><p>C: CPU burst - how long process runs for each period</p></li>
              <li><p>T: period - deadline (e.g. every 30 seconds)</p></li>
              <li><p>U: utilization = C/T</p></li>
              <li><p>Rate Monotonic Scheduling: select process with highest rate (1/period)</p></li>
              <li style="list-style-type: none";><p>- only works for periodic processes</p></li>
              <li style="list-style-type: none";><p>- if sum of utilizations &le; n(2<sup>1/n</sup>-1) then all deadlines will be met</p></li>
              <li style="list-style-type: none";><p>- preempt if new period starts and there is a higher priority process</p></li>
              <li style="list-style-type: none";><p>- if process completes burst, it goes to sleep until next period</p></li>
              <li style="list-style-type: none";><p>- static: priorities of processes don't change over time</p></li>
              <li style="list-style-type: none";><p>- RMS is optimal for static priority scheduling</p></li>
              <li style="list-style-type: none";><p>- no matter how many processes there are, if they're not asking for &gt; 69% of CPU, RMS will work</p></li>
            </ul>
          </div>
          <div class="content" id="synchronization">
            <h3>Synchronization</h3>
            <ul>
              <li><p>Synchronize: when events happen at the same time</p></li>
              <li><p>Process synchronization: when events in processes occur at the same time</p></li>
              <li style="list-style-type: none";><p>- every instruction, function call is an event</p></li>
              <li><p><b>Use synchronization to prevent race conditions and wait for resources</b></p></li>
              <li><p><b>Critical section: code that may be executing by different processes, but needs to be atomic: cannot be divided; cannot interleave critical sections that are related to each other</b></p></li>
              <li><p><b>Mutual exclusion: only 1 process active in a critical section</b></p></li>
              <li><p>Programmer must identify critical sections and surround them with entry/exit code, which makes processes atomic</p></li>
              <li><p>Entry code should allow processes to enter critical section if there are no other processes inside</p></li>
              <li style="list-style-type: none";><p>- none inside -> open; process enters -> close; process leaves -> open</p></li>
              <li><p>Requirements for a good solution for entry/exit code (mutual exclusion):</p></li>
              <li style="list-style-type: none";><p>- at most 1 process in critical section</p></li>
              <li style="list-style-type: none";><p>- can't block entry if no process inside critical section</p></li>
              <li style="list-style-type: none";><p>- a process that is waiting should be able to enter critical section</p></li>
              <li style="list-style-type: none";><p>- no assumptions about CPU speed or number</p></li>
            </ul>
            <h5>Software Lock</h5>
            <ul>
              <code>shared int lock = OPEN;</code>
              <div class="display_flex">
                <p>P<sub>1</sub></p>
                <code>while (lock == CLOSED);<br>lock = CLOSED;<br>&lt; critical section &gt;<br>lock = OPEN;</code>
                <p>P<sub>2</sub></p>
                <code>while (lock == CLOSED);<br>lock = CLOSED;<br>&lt; critical section &gt;<br>lock = OPEN;</code>
              </div>
              <li><p>At first, lock is open, so while loop will be skipped. Then lock will be closed. Then process will enter critical section. Since lock is closed, the other process will be stuck in the while loop, and thus, be unable to enter critical section. Once the process exits the critical section, the lock is open, and the other process will be able to enter the critical section.</p></li>
              <li><p>Problem: P<sub>1</sub> checks lock, lock is open so skip while loop, but just before it closes the lock, there is a context switch to P<sub>2</sub>. The lock is still open, so P<sub>2</sub> also skips the while loop and enters the critical section. There is a context switch back to P<sub>1</sub> and it also enters the critical section. Broke rule: at most 1 process in critical section</p></li>
            </ul>
            <h5>Take Turns</h5>
            <ul>
              <code>shared int turn = 0;</code>
              <div class="display_flex">
                <p>P<sub>1</sub></p>
                <code>while (turn != 0);<br>&lt; critical section &gt;<br>turn = 1;</code>
                <p>P<sub>2</sub></p>
                <code>while (turn != 1);<br>&lt; critical section &gt;<br>turn = 0;</code>
              </div>
              <li><p>At first, turn is 0, so P1 gets to go inside the critical section first. By doing this, P2 won't go inside the critical section since it is not P2's turn.</p></li>
              <li><p>Problem: if P2 is scheduled to go before P1, it will be stuck in the while loop and won't enter the critical section. Broke rule: can't block entry if no process inside critical section</p></li>
            </ul>
            <h5>State Intention</h5>
            <ul>
              <code>shared boolean intent[2] = {FALSE, FALSE};</code>
              <div class="display_flex">
                <p>P<sub>1</sub></p>
                <code>intent[0] = TRUE;<br>while (intent[1]);<br>&lt; critical section &gt;<br>intent[0] = FALSE;</code>
                <p>P<sub>2</sub></p>
                <code>intent[1] = TRUE;<br>while (intent[0]);<br>&lt; critical section &gt;<br>intent[1] = FALSE;</code>
              </div>
              <li><p>When a process starts, it will declare its intention to go into the critical section. But before it goes in, it checks if the other process was intending to go inside the critical section. If it was, it will let the other process go first.</p></li>
              <li><p>Problem: P<sub>1</sub> sets intent = TRUE, there is a context switch to P<sub>2</sub>, P<sub>2</sub> sets intent = TRUE, now both are stuck in the while loop (busy-waiting). Broke rule: a process that is waiting should be able to enter critical section</p></li>
            </ul>
            <h5>Peterson's Solution</h5>
            <ul>
              <code>shared int turn;<br>shared boolean intent[2] = {FALSE, FALSE};</code>
              <div class="display_flex">
                <p>P<sub>1</sub></p>
                <code>intent[0] = TRUE;<br>turn = 1;<br>while (intent[1] && turn == 1);<br>&lt; critical section &gt;<br>intent[0] = FALSE;</code>
                <p>P<sub>2</sub></p>
                <code>intent[1] = TRUE;<br>turn = 0;<br>while (intent[0] && turn == 0);<br>&lt; critical section &gt;<br>intent[1] = FALSE;</code>
              </div>
              <li><p>Combines take turns and state intention</p></li>
              <li><p>Problem: none!</p></li>
              <li><p>An all-software solution</p></li>
            </ul>
            <h5>Disable Interrupts</h5>
            <ul>
              <li><p>Context switching causes problems; interrupts cause context switching; so disable interrupts before entering critical sections and enable when leaving</p></li>
              <li><p>Problem: interrupts are turned off on a per-CPU basis. Broke rule: no assumptions about CPU speed or number</p></li>
            </ul>
            <h5>Test-and-Set Lock</h5>
            <ul>
              <li><p>Takes a memory location, tests whether its contents == 0 and sets contents = 1 atomically (virtually, at the same time)</p></li>
              <li><p>Typical hardware solution</p></li>
            </ul>
            <h5>Semaphores</h5>
            <ul>
              <li><p>Variable that helps us achieve synchronization</p></li>
              <li><p>Can declare and initialize it to some integer value</p></li>
              <li><p>Can call wait or call signal on it</p></li>
              <li style="list-style-type: none";><p>- wait will decrement value of semaphore and check if value < 0; if yes, block the currently running process, else, return</p></li>
              <li style="list-style-type: none";><p>- signal will increment value of semaphore and unblock 1 process, if any are blocked</p></li>
              <li><p>synchronization is more than mutual exclusion</p></li>
              <li><p>Can use semaphores to control order of process execution (want P<sub>1</sub> before P<sub>2</sub>)</p></li>
              <li style="list-style-type: none";><p>- conditional synchronization or general synchronization</p></li>
              <li><p>Semaphores only provide synchronization; they don't allow any information to be transferred from one process to another</p></li>
              <li style="list-style-type: none";><p>- A and B use semaphores; B can never learn anything about behavior of A (e.g. whether A is blocked)</p></li>
              <li><p>Can't check value of semaphore; only initialize, wait, signal</p></li>
              <li><p>Wait and signal must be atomic, so need TSL or Peterson's solution</p></li>
              <li><p>Synchronization: one process waiting for another one</p></li>
            </ul>
          </div>
          <div class="content" id="ipc">
            <h3>InterProcess Communication</h3>
            <ul>
              <li><p>Used when want to build multiprocess program</p></li>
              <li><p>Want to support cooperating processes for speed and modularity</p></li>
              <li style="list-style-type: none";><p>- speed: exploit parallelism; one process waits for resources and other ones can proceed</p></li>
              <li style="list-style-type: none";><p>- modularity: reusable, self-contained programs</p></li>
            </ul>
            <h5>Examples of Cooperating Processes</h5>
            <ul>
              <li><p>Pipeline: P<sub>1</sub> -> P<sub>2</sub> -> P<sub>3</sub> output of P<sub>1</sub> is input of P<sub>2</sub> ...</p></li>
              <li><p>Client/Server: client needs a service (result of a function), sends message to server, server computes answer and sends it back to client</p></li>
              <li style="list-style-type: none";><p>- good for cooperating processes built by different people (e.g. web browser and web server)</p></li>
              <li><p>Parent/Child: parent generates numerous children who computer subresults and parent combines results</p></li>
              <li><p>Cooperate: processes need to talk to each other to organize their activity</p></li>
              <li><p><b>IPC requires data transfer mechanism and synchronization mechanism</b></p></li>
              <li style="list-style-type: none";><p>- semaphores are not an IPC mechanism because no data transfer</p></li>
              <li><p>There are three abstractions for IPC: shared memory + semaphores, monitors, and message passing</p></li>
            </ul>
            <h5>Producer/Consumer Problem</h5>
            <ul>
              <li><p>Producer produces data; consumer consumes what was produced (both are processes)</p></li>
              <li><p>Work on a shared buffer (array)</p></li>
            </ul>
            <h5>Shared Memory + Semaphores</h5>
            <ul>
              <li><p>Shared variables to tell where things are in buffer</p></li>
              <li><p>Won't work if consumer goes first (nothing in buffer)</p></li>
              <li><p>Consumer has semaphore for waiting until there are filled slots</p></li>
              <li><p>Producer has semaphore for waiting until slots are empty before inputting</p></li>
              <li><p>Won't work for multiple producers; producers might overwrite values</p></li>
              <li style="list-style-type: none";><p>- inputting into array is a critical section; surround with additional semaphores</p></li>
              <li><p>One semaphore solves synchronization and the other solves mutual exclusion</p></li>
            </ul>
            <h5>Monitors</h5>
            <ul>
              <li><p>Programming language constructs; if programming langauge doesn't support monitors, can't use</p></li>
              <li><p>Consists of shared variables, accessed via procedures that make up the monitor</p></li>
              <li><p>Have condition variables for blocking and unblocking processes</p></li>
              <li style="list-style-type: none";><p>- wait: block; signal: unblock</p></li>
              <li><p>Only 1 process can be active inside a monitor</p></li>
              <li><p>Producer produces n times then blocks; consumer consumes n times then blocks</p></li>
              <li><p>No race condition because only 1 process active inside monitor</p></li>
              <li style="list-style-type: none";><p>- since monitors are a programming language construct, compiler adds code to make them atomic</p></li>
              <li style="list-style-type: none";><p>- semaphores are not a programming language construct</p></li>
              <li><p>Think of monitor as an apartment with 2 rooms (active area and waiting area) and 1 door (between active area and outside the apartment)</p></li>
              <li><p>When process calls monitor function, it is entering the monitor</p></li>
              <li style="list-style-type: none";><p>- door is open by default; closed when process enters</p></li>
              <li><p>If process is done, it exits and the door opens</p></li>
              <li><p>If process is inside monitor and calls wait, it moves from active area to waiting area; then door opens and another process can come in to the active area</p></li>
              <li><p>Condition variables have no memory; can't "remember" that wait/signal was called previously</p></li>
              <li><p>Monitors bring additional structure to IPC</p></li>
            </ul>
            <h5>Message Passing</h5>
            <ul>
              <li><p>Two functions: send and receive</p></li>
              <li style="list-style-type: none";><p>- send: give name of process and pointer to buffer</p></li>
              <li><p>Kernel copies data into kernel; records that it was from P<sub>1</sub> and supposed to go to P<sub>2</sub></p></li>
              <li style="list-style-type: none";><p>- happens when P<sub>1</sub> calls send, which is a system call</p></li>
              <li><p>If P<sub>2</sub> calls receive, kernel copies data into buffer</p></li>
              <li><p>How data transfer is implemented: kernel transfers data from one place to another</p></li>
              <li><p>How synchronization is achieved: process waits until message is sent before receiving</p></li>
              <li><p>No shared memory - good for multiprocess program over a network, multiple computers have no shared memory</p></li>
              <li><p>Kernel is in control, so it can block producer or not schedule it to run if producing too much</p></li>
              <li><p>Who should messages be sent to? Since the client and server may be two programs written by different people, how does client know process ID of server?</p></li>
              <li style="list-style-type: none";><p>- port: like a mailbox; has a number everyone recognizes</p></li>
              <li><p>Kernel buffering: messages haven't been sent yet, pile up in kernel</p></li>
              <li style="list-style-type: none";><p>- kernel could block/not allow to run process producing messages</p></li>
              <li><p>Safer than shared memory paradigms because shared memory can lead to programmer abuse/error</p></li>
            </ul>
          </div>
          <div class="content" id="deadlock">
            <h3>Deadlock</h3>
            <ul>
              <li><p><b>Set of processes that are permanently blocked; not able to run because of logical reason</b></p></li>
              <li style="list-style-type: none";><p>- <b>unblocking one process requires another to run, but no others can run</b></p></li>
              <li><p>Processes ask for resources and kernel gives them out; sometimes process holds on to resource and sometimes ask for resource</p></li>
              <li><p>Ex: memory = 200; P<sub>1</sub> asks for 80, memory = 120; P<sub>2</sub> asks for 70, memory = 50; P<sub>1</sub> asks for 60, P<sub>1</sub> gets blocked; P<sub>2</sub> asks for 80, P<sub>2</sub> gets blocked</p></li>
            </ul>
            <h5>Four Conditions For Deadlock</h5>
            <ul>
              <li><p><b>Deadlock is possible only if all four conditions are present:</b></p></li>
              <li style="list-style-type: none";><p>- mutual exclusion: only 1 process may use a resource at a time</p></li>
              <li style="list-style-type: none";><p>- hold-and-wait: process holds 1 resource and is waiting for another</p></li>
              <li style="list-style-type: none";><p>- no preemption: resources can't be taken away from a process</p></li>
              <li style="list-style-type: none";><p>- circular wait: hold-and-wait pattern is circular/cyclic</p></li>
            </ul>
            <h5>Attack the Deadlock Problem</h5>
            <ul>
              <li><p>Deadlock prevention: make deadlock impossible by removing 1 condition</p></li>
              <li><p>Deadlock avoidance: temporarily get rid of 1 condition, look at every situation and see if it's possible for a deadlock</p></li>
              <li><p>Deadlock detection: let them happen, maybe can recover</p></li>
            </ul>
            <h5>Deadlock Prevention</h5>
            <ul>
              <li><p>Remove mutual exclusion: why not allow all resources to be shared?</p></li>
              <li style="list-style-type: none";><p>- some resources can't be shared (e.g. printer, critical section)</p></li>
              <li><p>Remove hold-and-wait: what if we make processes ask for resources all at once? (e.g. if need resources A and B, need to ask for both at the same time, not A then later B)</p></li>
              <li style="list-style-type: none";><p>- processes don't know what they will want; may realize need more later</p></li>
              <li><p>Remove no preemption: what if we allow resources to be taken away?</p></li>
              <li style="list-style-type: none";><p>- bad for some things like printing; jobs need to be completed</p></li>
              <li><p>Remove circular wait: what if we numbered all resources and processes have to ask for them in order? (e.g. if process wants resources 3, 5, and 9, it has to get 3, then 5, then, 9; not 9, then 3, then 5</p></li>
              <li style="list-style-type: none";><p>- there are an unknown number of resources; some are created on demand</p></li>
            </ul>
            <h5>Deadlock Avoidance</h5>
            <ul>
              <li><p>Avoid getting into situations that lead to deadlock; remove condition only when deadlock is possible</p></li>
              <li><p>Works when resources are given out in increments (e.g. memory, CPU)</p></li>
              <li><p>Processes would have to declare upfront the max amount they need, but they often don't know how much they need</p></li>
              <li><p>If they did know, could use Banker's Algorithm</p></li>
            </ul>
            <h5>Banker's Algorithm</h5>
            <ul>
              <li><p>Works for fixed number of processes and resources</p></li>
              <li><p>Characterize system as either in a safe state or unsafe state</p></li>
              <li><p>Safe state: deadlock is avoidable</p></li>
              <li><p>Unsafe state: deadlock is possible; not certain</p></li>
              <li><p><b>System always begins in a safe state, everytime there is a request, kernel asks if giving resource makes system go from safe to unsafe; if so, won't grant request</b></p></li>
              <li><p>Would need 3 data structures:</p></li>
              <li style="list-style-type: none";><p>- process/resource claim matrix: what is the max amount a process may ask for</p></li>
              <li style="list-style-type: none";><p>- process/resource allocation matrix: what did they actually ask for and what do they currently own</p></li>
              <li style="list-style-type: none";><p>- resource availability vector: how many resources are available/free</p></li>
              <li><p>Can look at contents of these data structures and classify system as safe or unsafe</p></li>
              <li><p>Safe state means at least 1 process can run to completion</p></li>
              <li><p>If process may need more resources but none are available, don't run that process</p></li>
            </ul>
            <h5>Detection and Recovery</h5>
            <ul>
              <li><p>Do nothing to prevent/avoid deadlocks; if they happen, they happen</p></li>
              <li><p>Periodically try to see if deadlock did occur and try to do something about it</p></li>
              <li><p>Deadlocks rarely happen; why put all this complexity (of prevention and avoidance) into kernel for something unusual?</p></li>
              <li><p>Cost of putting them in may be very high</p></li>
              <li><p>Most general purpose operating systems (e.g. laptops) take this approach</p></li>
              <li><p>Detect deadlock by constructing resource allocation graph and see if there's a cycle</p></li>
              <li><p>Can kill all processes that are deadlocked</p></li>
              <li><p>Can terminate them one at a time and see if that solves it</p></li>
              <li style="list-style-type: none";><p>- process can be left in inconsistent state (writing/sending message may contain garbage)</p></li>
            </ul>
          </div>
          <div class="content" id="memory_management">
            <h3>Memory Management</h3>
            <ul>
              <li><p>Process has to store code (text), static variables (heap), activation records (stack)</p></li>
              <li><p>Text is fixed; stack and data grow/shrink</p></li>
              <li><p>What it would look like if process owns whole memory:</p></li>
              <img class="img-fluid" src="./pictures/text-data-stack (memory).jpg">
              <li style="list-style-type: none";><p>- process thinks it does own whole memory</p></li>
              <li><p>Address space: set of addresses needed to access memory</p></li>
              <li><p>Compiler has this view of memory as well</p></li>
              <li><p>Compiler generates all of the memory addresses for variables</p></li>
              <li><p><b>The problem is the compiler works with incomplete information; it doesn't know where the program will run in physical memory</b></p></li>
              <li><p>Typically there are many processes in physical memory; only 1 can live at location 0</p></li>
              <li><p>Compiler doesn't know that; <b>pretends that the process's memory goes from 0 to some max value and allocate addresses based on that</b></p></li>
              <li><p><b>The operating system figures out how to map those addresses into real physical addresses</b></p></li>
              <li><p>Abstract - each process needs its own CPU and memory</p></li>
              <li><p>Reality - single CPU whose time is divided up; single memory whose space is divided up</p></li>
              <li><p>Why do all processes have to live in memory?</p></li>
              <li><p>Consider an alternative - have all the processes live in a disk (secondary storage); when we want to run a process, we have to move its memory into physical memory</p></li>
              <li style="list-style-type: none";><p>- takes too long to load from/to disk <em>and</em> to perform a context switch</p></li>
              <li><p>The solution is to keep all processes inside memory so we just have to change context of CPU and move pointers around</p></li>
              <li><p><b>We have a number of processes and want to fit them into memory</b></p></li>
              <li><p>Memory starts empty; one big hole</p></li>
              <li><p>Over time, memory gets allocated into blocks (put process into block and put block into hole)</p></li>
              <img class="img-fluid" src="./pictures/memory_management.gif">
              <li><p><b>Every allocation requires finding a hole large enough to fit the process</b></p></li>
              <li><p>Usually, an allocation leaves a smaller hole</p></li>
              <li><p>When memory is not needed, it is released and holes next to each other combine</p></li>
            </ul>
            <h5>Selecting the Best Hole</h5>
            <ul>
              <li><p>First Fit: find the first hole that fits</p></li>
              <li style="list-style-type: none";><p>- simple, fast</p></li>
              <img class="img-fluid" src="./pictures/first_fit.gif">
              <li><p>Best Fit: find the hole that fits as perfectly as possible</p></li>
              <li style="list-style-type: none";><p>- have to look at every hole</p></li>
              <li style="list-style-type: none";><p>- leaves very small holes - fragments - that are likely unusable</p></li>
              <img class="img-fluid" src="./pictures/best_fit.gif">
              <li><p>Worst Fit: find the hole that leaves the largest resulting hole</p></li>
              <li style="list-style-type: none";><p>- have to check every hole</p></li>
              <img class="img-fluid" src="./pictures/worst_fit.gif">
              <li><p>Which one is the best?</p></li>
              <li style="list-style-type: none";><p>- consider tradeoff: how well blocks fit vs search time</p></li>
              <li><p><b>Prefer to waste memory space than waste CPU time</b></p></li>
              <li style="list-style-type: none";><p>- <b>memory is cheap; time is precious</b></p></li>
              <li><p>First Fit is typically used because it is fast</p></li>
              <li><p>Eventually, memory becomes fragmented - has unused holes</p></li>
              <li><p>Internal fragmentation: memory not being used inside block (room between process's stack and data)</p></li>
              <li style="list-style-type: none";><p>- unused memory inside process</p></li>
              <li><p>External fragmentation: outside of allocated regions; usable for new requests (processes)</p></li>
              <li style="list-style-type: none";><p>- unused memory inside overall memory</p></li>
              <li><p>Compaction: combine all external fragments into a big hole; hopefully it is large enough for another process</p></li>
              <li style="list-style-type: none";><p>- simple, but time consuming</p></li>
              <li><p>Break block (process's memory) into little pieces and fit those pieces into fragments</p></li>
              <li style="list-style-type: none";><p>- easier to fit, but complicated</p></li>
              <li><p>Prefer breaking over compaction; prefer complexity over time</p></li>
            </ul>
            <h5>50% Rule</h5>
            <ul>
              <li><p>On average, how many holes are there going to be?</p></li>
              <li><p>If there are n blocks allocated, there will be m = <sup>n</sup> &frasl; <sub>2</sub> holes, on average</p></li>
            </ul>
            <h5>Unused Memory Rule</h5>
            <ul>
              <li><p>On average, how much memory is lost to holes?</p></li>
              <li><p>b: average size of blocks; h: average size of holes; f: fraction of memory lost to holes</p></li>
              <li><p>M: size of memory; m: number of holes; n: number of blocks</p></li>
              <li><p>k = <sup>h</sup> &frasl; <sub>b</sub> (ratio of average hole-to-block size)</p></li>
              <li><p>M = mh + nb</p></li>
              <li><p>f = <sup>mh</sup> &frasl; <sub>M</sub> = <sup>mh</sup> &frasl; <sub>mh + nb</sub></p></li>
              <li><p>m = <sup>n</sup> &frasl; <sub>2</sub> -> n = 2m (by the 50% rule)</p></li>
              <li><p>f = <sup>mh</sup> &frasl; <sub>mh + 2mb</sub> = <sup>h</sup> &frasl; <sub>h + 2b</sub></p></li>
              <li><p>k = <sup>h</sup> &frasl; <sub>b</sub> -> h = kb</p></li>
              <li><p>f = <sup>kb</sup> &frasl; <sub>kb + 2b</sub> = <sup>k</sup> &frasl; <sub>k + 2</sub></p></li>
              <li><p>If k = 1, then the average hole size = average block size. Then f = 1/3, which means there is 33% wasted memory if the average hole size = average block size</p></li>
              <li><p>If k = 2, then the average hole size = 2x average block size. Then f = 2/4 = 1/2, which means there is 50% wasted memory if the average hole size = 2x average block size</p></li>
              <li><p>As k -> &infin;, f -> 1. This means the bigger the average hole size, the more wasted memory there is</p></li>
              <li><p>As k -> 0, f -> 1. This means the smaller the average hole size, the less wasted memory there is</p></li>
            </ul>
            <h5>Pre-sized Holes</h5>
            <ul>
              <li><p>Variable-sized allocations cause fragmentation</p></li>
              <li><p>So let's use pre-sized holes</p></li>
              <li><p>Make all holes the same size</p></li>
              <li style="list-style-type: none";><p>- easy to allocate blocks, but the holes may be too small</p></li>
              <li><p>Make holes in different sizes (e.g. small, medium, large)</p></li>
              <li style="list-style-type: none";><p>- more flexible, but complex</p></li>
            </ul>
            <h5>Buddy System</h5>
            <ul>
              <li><p>Partition the holes into sizes of powers of 2</p></li>
              <li><p>allocation: r = size of request</p></li>
              <li style="list-style-type: none";><p>- find hole larger than r</p></li>
              <li style="list-style-type: none";><p>- break hole into 2 buddies and see if half is ok; keep breaking until too small</p></li>
              <li><p>free: check if buddy is free and coalesce</p></li>
              <img class="img-fluid" src="./pictures/buddy_system.gif">
            </ul>
          </div>
          <div class="content" id="logical_memory">
            <h3>Logical Memory</h3>
            <ul>
              <li><p>There are three problems with sharing memory among processes</p></li>
              <li><p>Addressing problem: every process thinks they own their own memory and everyone's memories starts at 0</p></li>
              <li style="list-style-type: none";><p>- in physical memory, there is only one address 0 and only the kernel gets it</p></li>
              <li style="list-style-type: none";><p>- processes' memories don't actually start at 0</p></li>
              <li><p>Protection problem: because multiple processes live in memory, we don't want one process to touch the memory of another process</p></li>
              <li style="list-style-type: none";><p>- possible because we are forced to keep processes in memory</p></li>
              <li><p>Space problem: one physical memory shared among a lot of processes means each process only gets a little; even less as more processes come in</p></li>
              <li><p>Address space: set of addresses for memory</p></li>
              <li style="list-style-type: none";><p>- usually linear; starting at 0 and going to some max value n-1 where n is the size of memory</p></li>
              <li><p>Physical address space: address space that applies to physical memory</p></li>
              <li><p>Logical address: address for logical memory</p></li>
              <li><p>Logical memory: memory that belongs to process; process thinks it owns that memory; the compiler generates addresses for that memory and can reason about that memory</p></li>
              <li style="list-style-type: none";><p>- can say memory always starts at 0; structured a certain way; goes up by increments of x; independent of physical memory</p></li>
              <li><p><b>Machine doesn't care about logical memory, so we need to convert logical addresses to physical addresses</b></p></li>
              <li><p>There are two possible times when we can do that conversion</p></li>
              <li><p>Load time: when we load executable into memory; convert via software</p></li>
              <li style="list-style-type: none";><p>- ex: can add 100 to every address</p></li>
              <li style="list-style-type: none";><p>- doesn't work with dynamic allocation (creating new address not known at load time)</p></li>
              <li><p>Access time: every time memory is accessed, we can convert logical address at that point just before it's sent to memory; convert via hardware</p></li>
            </ul>
            <h5>Hardware for Logical Addressing</h5>
            <ul>
              <li><p>Base register: hardware register; when there is a context switch to a process, the base register gets filled with the start value of that process's memory; every time a logical address is supplied, it gets added by the hardware to the base register and that gets sent to the physical memory</p></li>
            </ul>
            <h5>Protection</h5>
            <ul>
              <li><p>Bound register: filled with size of logical memory</p></li>
              <li><p>Every time a logical address is supplied, it's checked against the bound register</p></li>
              <li style="list-style-type: none";><p>- is this address between 0 and n-1?</p></li>
              <li style="list-style-type: none";><p>- if yes, add it to the base register to get physical address</p></li>
              <li style="list-style-type: none";><p>- if no, it is an invalid address; hardware causes TRAP into kernel; kernel kills the process</p></li>
              <li><p>On every context switch, load the base and bound registers for the selected process</p></li>
              <li><p>Only the kernel can do this, so kernel memory has to be protected</p></li>
              <li style="list-style-type: none";><p>- advantage of putting kernel at 0 (or low address)</p></li>
              <li style="list-style-type: none";><p>- if process can't issue addresses that are negative, they can't reach behind themselves; kernel is behind all processes, so kernel is automatically protected</p></li>
            </ul>
            <h5>Fitting Processes Into Memory</h5>
            <ul>
              <li><p>Take advantage of the fact that a process is already broken up into text, data, stack; would like to put each of those into a hole that is available</p></li>
              <li><p>There are two approaches of breaking up memory into pieces</p></li>
              <li><p>Segmentation: break them up into variable size pieces that are fitted exactly for what we need; results in segmented address space; segments for text, data, stack</p></li>
              <li><p>Break them up into fixed size pieces regardless of what we actually need; results in paged address space</p></li>
              <li style="list-style-type: none";><p>- fixed size pieces are called pages</p></li>
              <li style="list-style-type: none";><p>- text will be same number of pages, data will be same number of pages, ...</p></li>
            </ul>
            <h5>Segmented Address Space</h5>
            <ul>
              <li><p>Address space is a set of segments</p></li>
              <li><p>Segment: linearly addressed memory; mini logical memory</p></li>
              <li style="list-style-type: none";><p>- typically contains logically-related information (e.g. all stuff in this segment is code, data, activation records, ...)</p></li>
              <li><p>Each segment is identified by an integer s, where 0 &le; s &le; S-1, S: max number of segments</p></li>
              <li><p>Hardware determines max number of segments</p></li>
              <li><p>Logical address is a pair of numbers (s, i), s: segment number, i: offset</p></li>
              <li style="list-style-type: none";><p>- which segment (denoted by s) and where within that segment (denoted by i)</p></li>
              <li><p>Convenient for compiler; it doesn't have to know anything about physical addresses</p></li>
              <li><p>No segment for hole between data and stack; not wasting logical memory</p></li>
              <li><p>How does hardware convert logical addresses based on segmentation into physical addresses?</p></li>
              <li><p>Segment table: describes how to convert segments into physical addresses that indicate the start of the segment in physical memory</p></li>
              <li><p>If we know where a segment starts in physical memory, we could just add the offset and get the physical address</p></li>
              <li><p>Physical address = ST(s) + i</p></li>
              <li><p>There is one segment table per process</p></li>
              <li><p>In each entry there are a set of fields:</p></li>
              <li style="list-style-type: none";><p>- valid bit: says whether that entry is valid</p></li>
              <li style="list-style-type: none";><p>- base: where segment is located in phsyical memory</p></li>
              <li style="list-style-type: none";><p>- bound: how big segment is</p></li>
              <li style="list-style-type: none";><p>- permissions: read, write, execute</p></li>
              <li><p>Table located in kernel because it has to be protected</p></li>
              <li><p>Hardware has to be told where table is located, because hardware uses it to automatically convert logical to physical addresses</p></li>
              <li><p>The kernel loads two hardware registers: segment table base register and segment table size register</p></li>
            </ul>
            <h5>Address Translation</h5>
            <ul>
              <li><p>Logical address: string of bits (bits for segment number followed by bits for offset)</p></li>
              <li><p>Hardware takes segment number, indexes into segment table, extracts the base value, and adds offset to get physical address</p></li>
              <li style="list-style-type: none";><p>- have to check if offset is within bound</p></li>
              <li><p>First check whether trying to access segment that is within segment table size</p></li>
              <li><p>Add segment number to segment table base value</p></li>
              <li><p>Check whether offset is within bounds</p></li>
              <li><p>Check permissions</p></li>
              <li><p>Add offset to base value</p></li>
            </ul>
            <h5>Sizing the Segment Table</h5>
            <ul>
              <li><p>How do we determine the size of the segment table?</p></li>
              <li><p>If there are n bits for the segment number in logical address, then there are 2<sup>n</sup> segments, so there are 2<sup>n</sup> entries in the table</p></li>
              <li><p>The width of the table is the number of bits in each entry</p></li>
              <li><p>Also need 1 bit for the valid bit</p></li>
              <li><p>The base value can reference any location in physical memory, so we need enough bits to reference any part of memory; we need to know size of memory, which will be the number of bits for the base</p></li>
              <li><p>Bound tells us size of a segment; need enough bits to express largest possible segment</p></li>
              <li style="list-style-type: none";><p>- offset indexes into segment; however many bits offset is, we can't reference any more than that within segment</p></li>
              <li style="list-style-type: none";><p>- number of bits for offset = max segment size = number of bits for bound</p></li>
              <li><p>Pad remaining space to make entry size a power of 2</p></li>
            </ul>
            <h5>Pros and Cons of Segmentation</h5>
            <ul>
              <li><p>Pros of segmentation:</p></li>
              <li style="list-style-type: none";><p>- each segment can be located independently; can put segments wherever we want because all we have to do is change the base value</p></li>
              <li style="list-style-type: none";><p>- can separately protect them because each segment can be bounded</p></li>
              <li style="list-style-type: none";><p>- can grow and shrink (increase/decrease bound)</p></li>
              <li style="list-style-type: none";><p>- segments can be shared by processes</p></li>
              <li style="list-style-type: none";><p>-- if everyone uses the same compiler, can store text at same address for everyone</p></li>
              <li><p>Cons of segmentation</p></li>
              <li style="list-style-type: none";><p>- variable size allocation is difficult to deal with</p></li>
              <li style="list-style-type: none";><p>-- hard to find holes</p></li>
              <li style="list-style-type: none";><p>-- external fragmentation</p></li>
            </ul>
            <h5>Paged Address Space</h5>
            <ul>
              <li><p>Break logical and physical memory into fixed-size pieces</p></li>
              <li><p>Pages of logical memory fit into frames of physical memory</p></li>
              <li><p>Frame: physical unit of information</p></li>
              <li><p>A page fits exactly into a frame; makes allocation easy</p></li>
              <li><p>Logical address: string of bits (bits for page number followed by bits for offset)</p></li>
              <li><p>Size of logical address space is (max number of pages) x (page size)</p></li>
            </ul>
            <h5>Frame-Based Physical Addressing</h5>
            <ul>
              <li><p>Form of physical address is frame number and offset (f, i)</p></li>
              <li><p>Offset is within a frame</p></li>
              <li><p>Translate logical address to physical: convert page number to frame number and concatenate with offset</p></li>
              <li><p>There is a page table, which is very similar to the segment table</p></li>
              <li><p>f = PT(p)</p></li>
              <li><p>Logical memory is a sequence of pages; physical memory is a sequence of frames</p></li>
              <li><p>Page table tells where to find pages in frames</p></li>
              <li><p>Page table is a sequence of entries with elements</p></li>
              <li style="list-style-type: none";><p>- valid bit; demand paging bits; frame number: which frame is this page located in; permission bits</p></li>
              <li><p>Page table is located in the kernel</p></li>
              <li><p>Tell hardware where the page table is located with:</p></li>
              <li style="list-style-type: none";><p>- page table base register: physical address of page table</p></li>
              <li style="list-style-type: none";><p>- page table size register: number of entries</p></li>
              <li><p>Hardware takes page number, checks to make sure it's valid to index into the page table by checking against the size register, then indexes into page table</p></li>
              <li><p>Then checks if valid bit is on, extracts frame number that tells us which frame in physical memory page is located, and concatenates with offset to get physical address</p></li>
            </ul>
            <h5>Sizing the Page Table</h5>
            <ul>
              <li><p>If there are n bits for page number, then there are 2<sup>n</sup> entries in the table</p></li>
              <li><p>How wide is each entry?</p></li>
              <li style="list-style-type: none";><p>- need to know how big physical memory is</p></li>
              <li style="list-style-type: none";><p>- need to know frame number</p></li>
              <li><p>To get frame number, take maximum physical memory size and divide it by the size of the frame</p></li>
              <li><p>Number of bits in offset is the size of the frame</p></li>
            </ul>
            <h5>Segments vs Pages</h5>
            <ul>
              <li><p>Segments are good logical units of information</p></li>
              <li><p>If we need to put all instructions in memory, it's nice to be able to say, "we need a memory of this size"</p></li>
              <li><p>Memory contains same kind of information</p></li>
              <li><p>However, different sizes lead to expensive allocations and segmentation</p></li>
              <li><p>Pages are good physical units of information</p></li>
              <li><p>Memory management is simple; any page will fit in any frame</p></li>
              <li><p>Can't size them to what is wanted; have to spread code over many pages</p></li>
            </ul>
            <h5>Combining Segments and Pages</h5>
            <ul>
              <li><p>Convert logical memory to physical memory by using segment tables and page tables</p></li>
              <li><p>A segment is a group of pages</p></li>
              <li><p>A segment table entry points to the page table</p></li>
              <li><p>Page table entry tells where to find page in what frame of memory</p></li>
              <li><p>Logical address has three fields: segment number, page number, offset</p></li>
              <li><p>Use segment number to index into segment table; base value in segment table tells where to find page table; use page number to index into page table; concatenate frame number of offset to get physical address</p></li>
              <li><p>Every memory reference costs 2 additional memory references</p></li>
              <li><p>Locality of reference: if we just referenced memory location 1000, there is a good chance that next memory reference will be the next address 1001</p></li>
              <li style="list-style-type: none";><p>- if we know page of 1000, then we know page of 1001; we only need to do translation once</p></li>
              <li><p>Add high-speed memory to keep cache of most frequent translations</p></li>
              <li><p>TLB: translation look-aside buffer</p></li>
              <li><p>Each entry is a pair of key and frame number</p></li>
              <li><p>Key is a bitstring of page number or segment+page number</p></li>
              <li><p>The more entries there are in the TLB, the higher the hit rate but the slower and more expensive memory references become</p></li>
              <li><p>TLB has to be flushed on every context switch</p></li>
              <li style="list-style-type: none";><p>- context switch changes which logical memory we are using; need new segment, page tables</p></li>
              <li><p>At the beginning of a quantum, all memory references are slow; later in the quantum, the TLB fills up, which makes the machine faster</p></li>
            </ul>
          </div>
          <div class="content" id="virtual_memory">
            <h3>Virtual Memory</h3>
          </div>
          <div class="content" id="file_system">
            <h3>File System</h3>
          </div>
          <div class="content" id="io">
            <h3>Input/Output System</h3>
          </div>
          <div class="content" id="protection">
            <h3>Protection</h3>
            <ul>
              <li><p>Processes need to access resources</p></li>
              <li><p>Resources are shared; want all processes to be able to access them</p></li>
              <li><p><b>Resources need to be protected</b></p></li>
              <li style="list-style-type: none";><p>- <b>don't want process to take it and use it forever</b></p></li>
              <li style="list-style-type: none";><p>- <b>don't want to use without permission</b></p></li>
              <li><p>The kernel enforces protection</p></li>
              <li><p>At the beginning, the kernel owns all the resources; then processes ask the kernel for resources</p></li>
              <li><p>Once a process is given access, the kernel can prevent others from gaining access; the kernel may or may not be able to take the resource away</p></li>
              <li><p>The kernel itself needs to be protected</p></li>
              <li><p>Typically, the kernel is protected by the hardware</p></li>
              <li><p><b>Protection: how to limit access to a resource</b></p></li>
              <li><p>Resource: anything that requires protection</p></li>
              <li><p>Domain: set of resource, permission pairs; where a process lives; defines what a process is allowed to do</p></li>
              <li><p>Process: the ones accessing resources</p></li>
            </ul>
            <h5>Protection Matrix</h5>
            <ul>
              <table>
                <tr>
                  <td class="no_border"></td>
                  <td class="no_border">A</td>
                  <td class="no_border">B</td>
                  <td class="no_border">C</td>
                  <td class="no_border">D</td>
                </tr>
                <tr>
                  <td class="no_border">X</td>
                  <td>r, w</td>
                  <td>r, w</td>
                  <td></td>
                  <td>w</td>
                </tr>
                <tr>
                  <td class="no_border">Y</td>
                  <td>w</td>
                  <td>r</td>
                  <td>r, w</td>
                  <td></td>
                </tr>
              </table>
              <li><p>Rows are domains</p></li>
              <li><p>Columns are resources</p></li>
              <li><p>Matrix entry contains permissions/rights (read, write)</p></li>
              <li><p>Inefficient because most of entries would be empty (since there are a lot of processes and resources)</p></li>
            </ul>
            <h5>Access Control Lists</h5>
            <ul>
              <li><p>Associated with resources; for every resource there is an access control list</p></li>
              <table>
                <tr>
                  <td class="no_border">ACL for B</td>
                </tr>
                <tr>
                  <td>X: r, w<br>Y: r</td>
                </tr>
              </table>
              <li><p>Checks if domain of process is on list; like a registry: if name is on list, then ok to access</p></li>
              <li><p>Inefficient because have to lookup on each resource access and have to check which domain</p></li>
              <li><p>However, easy to revoke access; just take name off list</p></li>
            </ul>
            <h5>Capability Lists</h5>
            <ul>
              <li><p>Associated with each domain</p></li>
              <table>
                <tr>
                  <td class="no_border">CL for X</td>
                </tr>
                <tr>
                  <td>A: r, w<br>B: r, w<br>D: w</td>
                </tr>
              </table>
              <li><p>Like key or ticket: if you have it, you get access</p></li>
              <li><p>Efficient because you just present list on each access</p></li>
              <li><p>However, it is hard to revoke access</p></li>
            </ul>
            <h5>Protection in UNIX</h5>
            <ul>
              <li><p>UNIX uses both access control lists and capability lists for resource protection</p></li>
              <li><p>The first time a process accesses a file, the kernel checks the process against the access control list. If the process has permission to open the file, the kernel gives the process a file descriptor. That file descriptor then acts like a capability list for future file accesses</p></li>
            </ul>
          </div>
          <div class="content" id="security">
            <h3>Security</h3>
          </div>
          <div class="content" id="networks">
            <h3>Networks</h3>
          </div>
          <div class="content" id="distributed_systems">
            <h3>Distributed Systems</h3>
          </div>
        </div>
      </div>
    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
  </body>
</html>
